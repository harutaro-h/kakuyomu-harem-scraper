# kakuyomu_harem_scraper.py
# -*- coding: utf-8 -*-
"""
Kakuyomu ã‚¿ã‚°ã€Œãƒãƒ¼ãƒ¬ãƒ ã€ã‚’äººæ°—é †ã§ãƒšãƒ¼ã‚¸é€ã‚Š (?page=2,3,...) ã—ãªãŒã‚‰å…¨å·¡å›ã€‚
å„ä½œå“ã«ã¤ã„ã¦:
- â˜…æ•°
- ç·æ–‡å­—æ•°
- æ³¨æ„æ›¸ãã€Œæ€§æå†™ã‚ã‚Šã€
- åˆå›å…¬é–‹æ—¥ï¼ˆç›®æ¬¡ã®æœ€å¤ <time datetime>ï¼‰
- ã‚¿ã‚°ï¼ˆä¸€è¦§/è©³ç´°ã®ä¸¡æ–¹ï¼‰
ã‚’å–å¾—ã—ã€ä»¥ä¸‹ã® AND æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ã—ãŸçµæœã‚’ CSV ã«å‡ºåŠ›ï¼ˆ0ä»¶ã§ã‚‚ãƒ˜ãƒƒãƒ€ä»˜ãï¼‰:

  åˆå›å…¬é–‹æ—¥ >= 2025-04-01
  â˜… >= 3000
  ã€Œæ€§æå†™ã‚ã‚Šã€
  æ–‡å­—æ•° >= 50000
  ã‚¿ã‚°ã«ã€Œãƒãƒ¼ãƒ¬ãƒ ã€ã‚’å«ã‚€ï¼ˆè¤‡åˆã‚¿ã‚°ã‚‚å¯ï¼‰

ã•ã‚‰ã« artifacts/ ã« HTMLãƒ»ã‚¹ã‚¯ã‚·ãƒ§ãƒ»ä¸æ•´åˆå€™è£œã‚’ä¿å­˜ï¼ˆActions ã® Artifacts ã§DLå¯ï¼‰ã€‚
"""

import os, re, csv, time
from pathlib import Path
from datetime import datetime
from typing import Optional, List
from urllib.parse import urlencode, urlsplit, urlunsplit, parse_qsl

from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_fixed

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# ===== è¨­å®š =====
BASE_URL = "https://kakuyomu.jp/tags/ãƒãƒ¼ãƒ¬ãƒ ?sort=popular"
OUT_CSV = "kakuyomu_harem_filtered.csv"
ARTIFACT_DIR = Path("artifacts"); ARTIFACT_DIR.mkdir(exist_ok=True)
MISMATCH_CSV = ARTIFACT_DIR / "mismatch.csv"

MIN_STARS = 3000
MIN_CHARS = 50000
MIN_DATE = datetime(2025, 4, 1)

MAX_PAGES = 50          # å®‰å…¨ä¸Šé™ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã™ï¼‰
PAGE_SLEEP = 1.0        # ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿ
DETAIL_SLEEP = 0.8      # ä½œå“è©³ç´°å–å¾—ã®å¾…æ©Ÿ
# =================


def get_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1366,768")
    ua = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
    opts.add_argument(f"--user-agent={ua}")
    chrome_path = os.environ.get("CHROME_PATH") or os.environ.get("CHROME_BIN")
    if chrome_path:
        opts.binary_location = chrome_path
    return webdriver.Chrome(options=opts)

def build_page_url(base: str, page: int) -> str:
    """BASE_URL ã« page ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä»˜åŠ /ä¸Šæ›¸ã"""
    u = urlsplit(base)
    q = dict(parse_qsl(u.query))
    q["page"] = str(page)
    new_q = urlencode(q, doseq=True)
    return urlunsplit((u.scheme, u.netloc, u.path, new_q, u.fragment))

def parse_number(s: Optional[str]) -> Optional[int]:
    if not s: return None
    m = re.sub(r"[^\d]", "", s)
    return int(m) if m.isdigit() else None

def pick_first_date_from_html(html: str) -> Optional[datetime]:
    soup = BeautifulSoup(html, "lxml")
    times = [t.get("datetime", "")[:10] for t in soup.select("time[datetime]")]
    dates: List[datetime] = []
    for d in times:
        try: dates.append(datetime.fromisoformat(d))
        except: pass
    return min(dates) if dates else None

@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def fetch_html(driver, url: str) -> str:
    driver.get(url)
    time.sleep(DETAIL_SLEEP)
    return driver.page_source

def card_select_all(html: str):
    soup = BeautifulSoup(html, "lxml")
    cards = soup.select("div.widget-workCard")
    if not cards:
        cards = [a.parent for a in soup.select("h3 a[href^='/works/']")]
    return soup, cards

def card_title_and_url(card) -> Optional[tuple]:
    a = card.select_one("h3 a[href^='/works/']") or card.select_one("a[href^='/works/']")
    if not a: return None
    return a.get_text(strip=True), "https://kakuyomu.jp" + a["href"]

def card_stars(card) -> Optional[int]:
    for sel in ["span.widget-workCard-reviewCount", "span.reviewCount", "[data-testid='review-count']"]:
        el = card.select_one(sel)
        if el and el.get_text(strip=True):
            return parse_number(el.get_text(strip=True))
    return None

def card_chars(card) -> Optional[int]:
    for sel in ["span.widget-workCard-charCount", "span.charCount", "[data-testid='char-count']"]:
        el = card.select_one(sel)
        if el and el.get_text(strip=True):
            return parse_number(el.get_text(strip=True))
    return None

def collect_tags(container) -> str:
    tags = []
    for sel in [".widget-workCard-tag", ".tag", "[data-testid='tag']"]:
        for t in container.select(sel):
            s = t.get_text(strip=True)
            if s: tags.append(s)
    return " ".join(tags)

def detect_sexual_notice(detail_soup: BeautifulSoup) -> bool:
    for sel in [".workHeader-notice", ".notice", "[data-testid='notice']"]:
        bloc = detail_soup.select(sel)
        if bloc:
            text = " ".join(el.get_text(" ", strip=True) for el in bloc)
            if "æ€§æå†™ã‚ã‚Š" in text: return True
    return "æ€§æå†™ã‚ã‚Š" in detail_soup.get_text(" ", strip=True)

def detail_fallbacks_if_missing(stars, chars, detail_soup):
    """ä¸€è¦§ã§æ¬ æã—ãŸå ´åˆã€è©³ç´°ãƒšãƒ¼ã‚¸å´ã§ã‚‚æ‹¾ãˆã‚‹ã‚‚ã®ã‚’æ‹¾ã†ï¼ˆå¼·åŒ–ç”¨ãƒ»å¿…è¦æœ€ä½é™ï¼‰"""
    # æ˜Ÿã¯è©³ç´°ãƒšãƒ¼ã‚¸ã§éœ²å‡ºãŒå¼±ã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ã€ãã®ã¾ã¾ None ã‚’è¨±å®¹
    # æ–‡å­—æ•°ã¯è©³ç´°ãƒšãƒ¼ã‚¸ã«åˆè¨ˆãŒå‡ºã‚‹ã“ã¨ãŒå¤šã„
    if chars is None:
        for sel in ["span.charCount", "[data-testid='char-count']"]:
            el = detail_soup.select_one(sel)
            if el and el.get_text(strip=True):
                chars = parse_number(el.get_text(strip=True))
                break
    return stars, chars

def main():
    driver = get_driver()
    all_rows = []
    mismatch_rows = []

    for page in range(1, MAX_PAGES + 1):
        page_url = build_page_url(BASE_URL, page)
        html = fetch_html(driver, page_url)
        (ARTIFACT_DIR / f"page_{page}.html").write_text(html, encoding="utf-8")
        driver.save_screenshot(str(ARTIFACT_DIR / f"page_{page}.png"))
        soup, cards = card_select_all(html)

        if not cards:
            print(f"page {page}: no cards -> stop")
            break

        print(f"page {page}: cards={len(cards)}")

        # å„ã‚«ãƒ¼ãƒ‰å‡¦ç†
        low_star_tail = 0
        for i, card in enumerate(cards, 1):
            tit_url = card_title_and_url(card)
            if not tit_url: continue
            title, work_url = tit_url

            stars = card_stars(card)
            chars = card_chars(card)
            tags_list = collect_tags(card)
            has_harem = "ãƒãƒ¼ãƒ¬ãƒ " in (tags_list or "")

            # ä½œå“è©³ç´°
            detail_html = fetch_html(driver, work_url)
            detail_soup = BeautifulSoup(detail_html, "lxml")
            notice = detect_sexual_notice(detail_soup)
            # ç›®æ¬¡ï¼ˆ/episodes å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒˆãƒƒãƒ—ã® <time> ç¾¤ã§ä»£ç”¨ï¼‰
            episodes_url = work_url + "/episodes"
            try:
                toc_html = fetch_html(driver, episodes_url)
            except Exception:
                toc_html = detail_html
            first_date = pick_first_date_from_html(toc_html)

            # æ¬ æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            stars, chars = detail_fallbacks_if_missing(stars, chars, detail_soup)
            tags_detail = collect_tags(detail_soup)

            # å³å¯†åˆ¤å®šï¼ˆæ¬ æã¯ Falseï¼‰
            meets = (
                isinstance(stars, int) and stars >= MIN_STARS and
                isinstance(chars, int) and chars >= MIN_CHARS and
                bool(notice) and bool(has_harem) and
                (first_date is not None and first_date >= MIN_DATE)
            )

            all_rows.append({
                "title": title,
                "url": work_url,
                "stars": stars if stars is not None else "",
                "total_chars": chars if chars is not None else "",
                "notice_sexual": notice,
                "first_episode_date": first_date.strftime("%Y-%m-%d") if first_date else "",
                "tags": tags_list,
                "tags_detail": tags_detail,
                "meets_conditions": meets
            })

            # æ˜Ÿã®ä½ã„é ˜åŸŸãŒç¶šã„ãŸã‚‰æ—©æœŸæ‰“ã¡åˆ‡ã‚Šã®ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰
            if isinstance(stars, int) and stars < MIN_STARS:
                low_star_tail += 1
            else:
                low_star_tail = 0

        # æœ«å°¾ã«â˜…<3000 ãŒå¤§é‡ã«ç¶šããƒšãƒ¼ã‚¸ãŒç¾ã‚ŒãŸã‚‰æ‰“ã¡åˆ‡ã‚‹ï¼ˆä»»æ„ï¼‰
        if low_star_tail >= max(10, len(cards)//2):
            print(f"page {page}: low-star tail detected -> early stop")
            break

        time.sleep(PAGE_SLEEP)

    driver.quit()

    # CSVï¼ˆå¿…ãšå‡ºåŠ›ï¼‰
headers = ["title","url","stars","total_chars","notice_sexual","first_episode_date","tags","tags_detail","meets_conditions"]

# ğŸ‘‡ è¿½åŠ ã“ã“ã‹ã‚‰
OUT_CSV = Path.cwd() / OUT_CSV
ARTIFACT_DIR = Path.cwd() / ARTIFACT_DIR
# ğŸ‘† è¿½åŠ ã“ã“ã¾ã§

with open(OUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
    w = csv.DictWriter(f, fieldnames=headers)
    w.writeheader()
    w.writerows(all_rows)

# â€œç–‘ã„ã‚ã‚Šâ€ã ã‘ã¾ã¨ã‚ï¼ˆæ¬ æ/ã‚¿ã‚°æ¬ è½/æ—¥ä»˜ãªã—ç­‰ï¼‰
for r in all_rows:
    if (r["stars"] == "" or r["total_chars"] == "" or not r["notice_sexual"] or "ãƒãƒ¼ãƒ¬ãƒ " not in str(r["tags"]) or r["first_episode_date"] == ""):
        mismatch_rows.append(r)
if mismatch_rows:
    with open(ARTIFACT_DIR / "mismatch.csv", "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=headers)
        w.writeheader()
        w.writerows(mismatch_rows)

# ã‚µãƒãƒª
(ARTIFACT_DIR / "scrape_summary.txt").write_text(
    f"pages_processed={page}\nrows={len(all_rows)}\n", encoding="utf-8"
)
print(f"Saved: {OUT_CSV} rows={len(all_rows)} mismatches={len(mismatch_rows)}")

